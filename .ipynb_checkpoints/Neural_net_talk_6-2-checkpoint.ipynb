{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Nets: The Devil's in the Details\n",
    "\n",
    "Seth Weidman    \n",
    "\n",
    "06/02/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Nets: WTF is going on??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='img/neural_net_wtf.png' height=200>\n",
    "\n",
    "What does this actually mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review from last time (Pt. 1)\n",
    "\n",
    "* Neural nets are nested functions, where the functions applied to the inputs alternate between two types:\n",
    "    * Linear functions, represented by matrix multiplications\n",
    "    * Non linear \"activation\" functions\n",
    "* Because they are functions, we can represent the neural net making a prediction mathematically. For example, if $X$ is the input, then the prediction $P$ is just: \n",
    "\n",
    "$$ P = A(B(C(D(X, V)), W)) $$\n",
    "\n",
    "where $A$, $B$, $C$, and $D$ are functions and $V$ and $W$ are weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Review from last time (Pt. 2)\n",
    "\n",
    "* This prediction is compared to the actual value we were trying to predict, $Y$, and a loss is computed, for example:\n",
    "\n",
    "$$ L = (Y - P) ^ 2 $$\n",
    "\n",
    "* And then the weights are adjusted so that this loss will be reduced:\n",
    "\n",
    "$$ W = W - \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial L}{\\partial V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key point from last time\n",
    "\n",
    "* This process - of computing derivatives to continually update the weights in a neural net - works because of **the chain rule from calculus**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can do basic neural nets!\n",
    "\n",
    "<img src=\"img/neural_net_check.png\">\n",
    "\n",
    "We got this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This basic neural net framework can learn MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**This basic neural net framework can learn MNIST**\n",
    "\n",
    "## Pt. 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**This basic neural net framework can learn MNIST**\n",
    "\n",
    "## Pt. 2: Read in the MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Read in the MNIST Data\n",
    "\n",
    "mnist = fetch_mldata('MNIST original') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_mnist_X_Y(mnist):\n",
    "    data = mnist.data\n",
    "    X = (data - data.min()) * 1.0 / (data.max() - data.min())\n",
    "    target = mnist.target\n",
    "    Y = np.zeros((len(target), 10))\n",
    "    for i in range(len(target)):\n",
    "        Y[i][int(target[i])] = 1 \n",
    "    print(\"Number of images: \", X.shape[0])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images:  70000\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_mnist_X_Y(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_prop = 0.9\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=1-train_prop, \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**This basic neural net framework can learn MNIST**\n",
    "\n",
    "## Pt. 3: Visualize the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_image(index):\n",
    "    target = mnist.target\n",
    "    print(\"Label: \", int(target[index]))\n",
    "    plt.imshow(1.0 - X[index].reshape(28,28), cmap='gray')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0ERfiCo/wfKBnT2y+G31wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13366d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**This basic neural net framework can learn MNIST**\n",
    "\n",
    "## Pt. 4: Train the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def _sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learn(X, Y, num_iter):\n",
    "    np.random.seed(2)\n",
    "    V = np.random.randn(784, 50)\n",
    "    W = np.random.randn(50, 10)\n",
    "    for j in range(num_iter):\n",
    "        i = np.random.randint(0, num_iter)\n",
    "        x = np.array(X[i], ndmin=2)\n",
    "        y = np.array(Y[i], ndmin=2)\n",
    "        A = np.dot(x,V)\n",
    "        B = _sigmoid(A)\n",
    "        C = np.dot(B,W)\n",
    "        P = _sigmoid(C)\n",
    "        sum_P = np.sum(P)\n",
    "        L = 0.5 * (y - P) ** 2\n",
    "        dLdP = -1.0 * (y-P)\n",
    "        dPdC = _sigmoid(C) * (1-_sigmoid(C))\n",
    "        dLdC = dLdP * dPdC\n",
    "        dCdW = B.T\n",
    "        dLdW = np.dot(dCdW, dLdC)\n",
    "        dCdB = W.T\n",
    "        dLdB = np.dot(dLdC, dCdB)\n",
    "        dBdA = _sigmoid(A) * (1-_sigmoid(A))\n",
    "        dLdA = dLdB * dBdA\n",
    "        dAdV = x.T\n",
    "        dLdV = np.dot(dAdV, dLdA)\n",
    "        W -= dLdW\n",
    "        V -= dLdV\n",
    "    return V, W  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "  def predict(X, V, W):\n",
    "    A = np.dot(X,V)\n",
    "    B = _sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = _sigmoid(C)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "V, W = learn(X_train, Y_train, num_iter=X_train.shape[0])\n",
    "P = predict(X_test, V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net MNIST Classification Accuracy: 91.3 percent\n"
     ]
    }
   ],
   "source": [
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "91.3% accuracy. Good..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...but not optimal: [see here](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)\n",
    "\n",
    "<img src='img/MNIST_performance.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Building deeper nets\n",
    "* Changing \"learning rates\" / using \"learning rate decay\"\n",
    "* Adding \"dropout\"\n",
    "* ...and that doesn't even cover many of the cutting edge techniques listed above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Are you overwhelmed yet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Paradox of choice\n",
    "\n",
    "<img src=\"supermarket.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Neural nets are really hard to unpack and understand - so much so that a lot of people don't even try.\n",
    "\n",
    "**By \"playing with these knobs\" individually - as well as learning a bit of theory, we can \"peek under the hood\" and understand how to make our neural nets work better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiple hidden layers (\"Deep Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**\"What I cannot build, I cannot understand.\"**\n",
    "\n",
    "--Richard Feynman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Before:\n",
    "\n",
    "With a neural net with one hidden layer, one pass through was 25 manually coded steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_This will quickly get unweildy if we add more hidden layers._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## New understanding: \"Layers\"\n",
    "\n",
    "No longer will we think of neural nets as complicated mathematical functions like $ P = A(B(C(D(X, V)), W)) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### New understanding: \"Layers\"\n",
    "\n",
    "Instead, we'll think of them as a series of layers:\n",
    "\n",
    "<img src='img/neural_net_layers.png' height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll define a `NeuralNetwork` class that defines a neural network as a series of `Layers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def _setup(self, input_shape):\n",
    "        \"\"\" Setup layer with parameters that are unknown at __init__(). \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Calculate layer output for given input (forward propagation). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient. \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Props\n",
    "\n",
    "<img src=\"img/andersbll.png\">\n",
    "\n",
    "[Anders' GitHub](https://github.com/andersbll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        weight_update = np.dot(dActivationOutputdActivationInput, dLayerInputdActivationInput)\n",
    "        W_new = self.W - weight_update\n",
    "        self.W = W_new\n",
    "        self.iteration += 1\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Coding the new neural network, layers - and don't forget about that activation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, bprop=False):\n",
    "    if bprop:\n",
    "        s = sigmoid(x)\n",
    "        return s*(1-s)\n",
    "    else:\n",
    "        return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running this neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer1 = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "layer2 = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist = NeuralNetwork(\n",
    "    layers=[layer1, layer2],\n",
    "    random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def neural_net_pass(net, x, y):\n",
    "    pred = net.forwardpass(x)\n",
    "    loss = net.loss(pred, y)\n",
    "    net.backpropogate(loss)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running this neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle the indices of the points in the training set:\n",
    "np.random.seed(4)\n",
    "train_size = X_train.shape[0]\n",
    "indices = list(range(train_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through every element in the training set: \n",
    "for index in indices:\n",
    "    x = np.array(X_train[index], ndmin=2)\n",
    "    y = np.array(Y_train[index], ndmin=2)\n",
    "    neural_net_pass(nn_mnist, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net MNIST Classification Accuracy: 82.3 percent\n"
     ]
    }
   ],
   "source": [
    "P = nn_mnist.forwardpass(X_test)\n",
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A different random weight initialization gave us just 82.3% accuracy this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Running this neural network\n",
    "\n",
    "#### Turning what we did above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deeper networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we've built a framework allowing us to define networks as a series of layers, we can easily build deeper networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "hidden_2 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "hidden_3 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def print_accuracy_net_layers(layers):\n",
    "    # Define net\n",
    "    net = NeuralNetwork(\n",
    "        layers=layers,\n",
    "        random_seed=2)\n",
    "\n",
    "    accuracy_one_hidden = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    return accuracy_one_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deeper networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with one hidden layer was 82.3\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with one hidden layer was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with two hidden layers was 90.7\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with two hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with three hidden layers was 88.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      hidden_2,\n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with three hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with four hidden layers was 87.7\n"
     ]
    }
   ],
   "source": [
    "accuracy = print_accuracy_net_layers([input_layer, \n",
    "                                      hidden_1, \n",
    "                                      hidden_2,\n",
    "                                      hidden_3,\n",
    "                                      output_layer])\n",
    "print(\"The accuracy of a network with four hidden layers was\", np.round(accuracy * 100.0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**\"Deep Learning\" can help, but we need more \"tricks\" to really get these things working.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/bengio.png\">\n",
    "\n",
    "\"The learning rate is the single most important hyperparameter and one should always make sure it is tuned.\"\n",
    "\n",
    "-[Yoshua Bengio](http://www.iro.umontreal.ca/~bengioy/yoshua_en/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            setattr(layer, 'learning_rate', self.learning_rate)\n",
    "            layer.initialize_weights()\n",
    "\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        weight_update = np.dot(dActivationOutputdActivationInput, dLayerInputdActivationInput)\n",
    "        W_new = self.W - self.learning_rate * weight_update\n",
    "        self.W = W_new\n",
    "        self.iteration += 1\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    print(\"The accuracy of a network with learning rate\", learning_rate, \"was\", np.round(accuracy_lr * 100.0, 1))\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with learning rate 0.0001 was 9.7\n",
      "The accuracy of a network with learning rate 0.001 was 18.1\n",
      "The accuracy of a network with learning rate 0.01 was 68.6\n",
      "The accuracy of a network with learning rate 0.1 was 89.8\n",
      "The accuracy of a network with learning rate 1 was 90.7\n",
      "The accuracy of a network with learning rate 10 was 17.9\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "lr_accuracies = [learning_rate_accuracy(x) for x in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because backpropogation involves multiplying a value by the derivative of the activation function, gradients (that tell the weights how to update) get smaller and smaller as you go get further from the output layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/sigmoid_deriv_trask.png\">\n",
    "**At most, the gradient can be multiplied by 0.25 at each layer.** More [here](http://iamtrask.github.io/2015/07/12/basic-python-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, \n",
    "                 random_seed, \n",
    "                 learning_rate):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            setattr(layer, 'learning_rate', \n",
    "                    self.learning_rate/(10.0 ** i))\n",
    "            layer.initialize_weights()\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with learning rate \\n\", learning_rate,\n",
    "          \"in the first layer \\n\", learning_rate / 10.0,\n",
    "          \"in the second layer, and \\n\", learning_rate / 100.0, \n",
    "          \"in the third layer is\", \n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate tuning: getting fancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with learning rate \n",
      " 0.01 in the first layer \n",
      " 0.001 in the second layer, and \n",
      " 0.0001 in the third layer is 9.7 percent\n",
      "The accuracy of a network with learning rate \n",
      " 0.1 in the first layer \n",
      " 0.01 in the second layer, and \n",
      " 0.001 in the third layer is 83.2 percent\n",
      "The accuracy of a network with learning rate \n",
      " 1 in the first layer \n",
      " 0.1 in the second layer, and \n",
      " 0.01 in the third layer is 91.8 percent\n",
      "The accuracy of a network with learning rate \n",
      " 10 in the first layer \n",
      " 1.0 in the second layer, and \n",
      " 0.1 in the third layer is 84.3 percent\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.1, 1, 10]\n",
    "lr_accuracies = [learning_rate_accuracy(x) for x in learning_rates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The accuracy is up to 91.8%, up from 90.7% before. We could also view that as roughly a 10% reduction in error rate. \n",
    "\n",
    "**Having a higher learning rate in earlier layers vs. later layers does help accuracy.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The weights in a neural net are updated according to:\n",
    "\n",
    "$$ W =  W - \\frac{\\partial L}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that this is equivalent to doing gradient descent with each parameter:\n",
    "\n",
    "<img src=\"img/gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is analogous to a ball rolling down a hill. \n",
    "\n",
    "Balls rolling down hills have momentum. So, therefore, should our weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate momentum\n",
    "\n",
    "Let's define our weight update $ \\frac{\\partial L}{\\partial W} $ to be $ U_t $. Then, instead of our weight update being $ U_t $ at each time step, it will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ U_t + \\mu * U_{t-1} + \\mu^2 * U_{t-2} + ... $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $\\mu$ is a decay parameter between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to, and often described as, increasing your learning rate when your weight updates are going in the same direction, iteration after iteration, and lowering your learning rate when the opposite is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    momentum = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = None\n",
    "        self.activation_function = activation_function\n",
    "        self.velocity = None\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "        self.velocity = np.zeros(shape=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, \n",
    "                                                        bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        \n",
    "        # Update velocity\n",
    "        weight_update_current = np.dot(dActivationOutputdActivationInput, \n",
    "                                       dLayerInputdActivationInput)\n",
    "        self.velocity = np.add(self.momentum * self.velocity, \n",
    "                               self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, learning_rate, momentum):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.learning_rate = learning_rate\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementing learning rate momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_accuracy(learning_rate, momentum):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with learning rate \\n\", learning_rate,\n",
    "          \"in the first layer \\n\", learning_rate / 10.0,\n",
    "          \"in the second layer, and \\n\", learning_rate / 100.0, \n",
    "          \"in the third layer, and \\n momentum\", momentum, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rate momentum results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with learning rate \n",
      " 1 in the first layer \n",
      " 0.1 in the second layer, and \n",
      " 0.01 in the third layer, and \n",
      " momentum 0.5 is 92.0 percent\n",
      "The accuracy of a network with learning rate \n",
      " 1 in the first layer \n",
      " 0.1 in the second layer, and \n",
      " 0.01 in the third layer, and \n",
      " momentum 0.75 is 90.2 percent\n",
      "The accuracy of a network with learning rate \n",
      " 1 in the first layer \n",
      " 0.1 in the second layer, and \n",
      " 0.01 in the third layer, and \n",
      " momentum 0.9 is 84.6 percent\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1]\n",
    "momentum = [0.5, 0.75, 0.9]\n",
    "for learning_rate in learning_rates:\n",
    "    for m in momentum:\n",
    "        learning_rate_accuracy(learning_rate, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, we get a practical lesson: **throwing the \"kitchen sink\" of neural net tricks at a problem in unnecessary at best and actually harmful at worst.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Indeed, learning rate momentum has been shown to work on Recurrent Neural Nets, not fully connected neural nets as we have here. See \n",
    "[Bengio et. al. (2014)](https://arxiv.org/pdf/1212.0901v2.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dropout can help prevent neural networks from overfitting. It involves \"dropping\" a portion of the neurons - that is, setting their values to zero - on each forward pass through the network. \n",
    "\n",
    "<img src=\"img/dropout.png\">\n",
    "\n",
    "This nudges the network toward learning \"redundant representations of its data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetworkDropout(object):\n",
    "    def __init__(self, layers, random_seed, \n",
    "                 learning_rate, momentum, dropout):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            np.random.seed(seed=self.random_seed+i*self.iteration)\n",
    "            if self.dropout:\n",
    "                zero_indices = np.random.choice(range(layer.n_in), \n",
    "                                                size=int(layer.n_in * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        self.iteration += 1\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net_dropout(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    net.dropout = None\n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_dropout(dropout):\n",
    "    \n",
    "    net = NeuralNetworkDropout(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = 1,\n",
    "        momentum = 0.5,\n",
    "        dropout = dropout\n",
    "    )\n",
    "    \n",
    "    accuracy_lr = train_test_net_dropout(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with dropout\", dropout, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of a network with dropout 1 is 92.0 percent\n",
      "The accuracy of a network with dropout 0.5 is 20.5 percent\n",
      "The accuracy of a network with dropout 0.75 is 88.0 percent\n",
      "The accuracy of a network with dropout 0.9 is 91.0 percent\n"
     ]
    }
   ],
   "source": [
    "dropouts = [1, 0.5, 0.75, 0.9]\n",
    "lr_accuracies = [learning_rate_dropout(x) for x in dropouts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Adding dropout did _not_ improve accuracy.** Our network was not overfitting in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Last one, just for fun: DropConnect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we saw above, the highest performance model on the MNIST data involved \"Drop Connect\", where a portion of the _weights_ in the neural net are set to zero, as opposed to half the _neurons_.\n",
    "\n",
    "[Not in TensorFlow!](https://stackoverflow.com/questions/37135885/dropconnect-in-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/drop_connect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, random_seed, \n",
    "                 learning_rate, momentum, drop_connect,\n",
    "                 dropout=None):\n",
    "        self.layers = layers\n",
    "        self.random_seed = random_seed\n",
    "        self.momentum = momentum\n",
    "        self.dropout = dropout\n",
    "        self.drop_connect = drop_connect\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            setattr(layer, 'random_seed', self.random_seed+i)\n",
    "            layer.initialize_weights()\n",
    "            setattr(layer, 'learning_rate', self.learning_rate/(10.0 ** i))\n",
    "            setattr(layer, 'momentum', self.momentum)\n",
    "            setattr(layer, 'drop_connect', self.drop_connect)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            np.random.seed(seed=self.random_seed+i*self.iteration)\n",
    "            if self.dropout:\n",
    "                zero_indices = np.random.choice(range(layer.n_in), \n",
    "                                                size=int(layer.n_in * (1 - self.dropout)), \n",
    "                                                replace=False)\n",
    "                X_next[:, zero_indices] = 0.0\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        self.iteration += 1\n",
    "        return prediction\n",
    "\n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        loss = 0.5 * (Y - prediction) ** 2\n",
    "        return -1.0 * (Y - prediction)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def apply_drop_connect_weights(weights, drop_connect):\n",
    "    new_weights = weights.copy()\n",
    "    num_weights = new_weights.shape[0] * new_weights.shape[1]\n",
    "    reshaped_weights = np.reshape(new_weights, (num_weights, 1))\n",
    "    zero_indices = np.random.choice(range(num_weights), \n",
    "                                    size=int(num_weights * (1 - drop_connect)), \n",
    "                                    replace=False)\n",
    "    reshaped_weights[zero_indices, :] = 0.0\n",
    "    drop_connected_weights = np.reshape(reshaped_weights, new_weights.shape)\n",
    "    \n",
    "    return drop_connected_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    learning_rate = None\n",
    "    momentum = None\n",
    "    drop_connect = None\n",
    "    \n",
    "    def __init__(self, n_in, n_out, \n",
    "                 activation_function):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out        \n",
    "        self.iteration = 0\n",
    "        self.activation_function = activation_function\n",
    "        self.velocity = None\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        self.W = np.random.normal(size=(self.n_in, self.n_out))\n",
    "        self.velocity = np.zeros(shape=(self.n_in, self.n_out))\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        if self.drop_connect:            \n",
    "            drop_connected_weights = apply_drop_connect_weights(self.W, \n",
    "                                                                self.drop_connect)\n",
    "            self.activation_input = np.dot(layer_input, \n",
    "                                           drop_connected_weights)\n",
    "        else:\n",
    "            self.activation_input = np.dot(layer_input, self.W)\n",
    "        self.iteration += 1\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, \n",
    "                                                        bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        \n",
    "        # Update velocity\n",
    "        weight_update_current = np.dot(dActivationOutputdActivationInput, \n",
    "                                       dLayerInputdActivationInput)\n",
    "        self.velocity = np.add(self.momentum * self.velocity, \n",
    "                               self.learning_rate * weight_update_current)\n",
    "        self.W = self.W - self.velocity\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_net_drop_connect(net, X_train, Y_train, X_test, Y_test, random_seed):\n",
    "\n",
    "    # Randomly set the seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Randomly shuffle the indices of the points in the training set:\n",
    "    train_size = X_train.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Loop through every element in the training set: \n",
    "    for index in indices:\n",
    "        x = np.array(X_train[index], ndmin=2)\n",
    "        y = np.array(Y_train[index], ndmin=2)\n",
    "        neural_net_pass(net, x, y)\n",
    "        \n",
    "    net.drop_connect = None\n",
    "    P = net.forwardpass(X_test)\n",
    "    preds = [np.argmax(x) for x in P]\n",
    "    actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "    accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_drop_connect(drop_connect):\n",
    "    \n",
    "    net = NeuralNetwork(\n",
    "        layers = [input_layer, hidden_1, output_layer],\n",
    "        random_seed = 2,\n",
    "        learning_rate = 1,\n",
    "        momentum = 0.5,\n",
    "        drop_connect = drop_connect\n",
    "    )\n",
    "\n",
    "    accuracy_lr = train_test_net_drop_connect(net, X_train, Y_train, X_test, Y_test, 4)\n",
    "    \n",
    "    print(\"The accuracy of a network with drop_connect\", \n",
    "          drop_connect, \"is\",\n",
    "          np.round(accuracy_lr * 100.0, 1), \"percent\")\n",
    "\n",
    "    return accuracy_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Linear(n_in=784, n_out=50, activation_function=sigmoid)\n",
    "hidden_1 = Linear(n_in=50, n_out=50, activation_function=sigmoid)\n",
    "output_layer = Linear(n_in=50, n_out=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## DropConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "drop_connect_values = [1, 0.5, 0.75, 0.9]\n",
    "lr_accuracies = [learning_rate_drop_connect(x) for x in drop_connect_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This network, not being convolutional, is likely _underfitting_ rather than _overfitting_ the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are a lot of \"tricks\" to improve the training of neural nets. **Now you know not just conceptually what those tricks are doing, but how to implement them.** You may even be able to implement a few of your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll examine:\n",
    "* Different activation functions! (here we only used boring ol' sigmoid)\n",
    "* Different weight initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Convolutional layers (yes, from scratch)\n",
    "* Recurrent neural nets (including LSTMs, omg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next steps for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Go to [the GitHub repo for this talk](https://github.com/SethHWeidman/neural_net_talk_6-2-17)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Star, fork, and contribute!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One more thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Jeremy and I are launching a podcast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tentative title: \"The Neural Net Guys\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some of you may be guests one day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No, seriously, we need guests, so if you could help out that would be great."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
